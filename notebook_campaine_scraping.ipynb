{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "burn_areas = gpd.read_file('fire-ignitions/campania/campania_burn_areas_2007_2021.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gid</th>\n",
       "      <th>loc</th>\n",
       "      <th>data_inc</th>\n",
       "      <th>ora_ev</th>\n",
       "      <th>data_ril</th>\n",
       "      <th>anno</th>\n",
       "      <th>sup_calc</th>\n",
       "      <th>supcalcb</th>\n",
       "      <th>supcalcnb</th>\n",
       "      <th>area_tot</th>\n",
       "      <th>date_form</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Centaurino-Rupe S. Paolo</td>\n",
       "      <td>06/01/2013</td>\n",
       "      <td>10:00</td>\n",
       "      <td>04/04/2013</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>150074.0</td>\n",
       "      <td>150074.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0074</td>\n",
       "      <td>dd/MM/yyyy</td>\n",
       "      <td>POLYGON ((1049568.528 4470259.565, 1049566.3 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SCANNI</td>\n",
       "      <td>03/03/2013</td>\n",
       "      <td>14:00</td>\n",
       "      <td>17/04/2013</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2884.0</td>\n",
       "      <td>2884.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2884</td>\n",
       "      <td>dd/MM/yyyy</td>\n",
       "      <td>POLYGON ((1032366.968 4476593.384, 1032377.892...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gid                       loc    data_inc ora_ev    data_ril    anno  \\\n",
       "0    1  Centaurino-Rupe S. Paolo  06/01/2013  10:00  04/04/2013  2013.0   \n",
       "1    2                    SCANNI  03/03/2013  14:00  17/04/2013  2013.0   \n",
       "\n",
       "   sup_calc  supcalcb  supcalcnb  area_tot   date_form  \\\n",
       "0  150074.0  150074.0        0.0   15.0074  dd/MM/yyyy   \n",
       "1    2884.0    2884.0        0.0    0.2884  dd/MM/yyyy   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((1049568.528 4470259.565, 1049566.3 4...  \n",
       "1  POLYGON ((1032366.968 4476593.384, 1032377.892...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burn_areas.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Headers: ['Temperature', 'Max', 'Average', 'Min', 'Dew Point', 'Max', 'Average', 'Min', 'Precipitation', 'Max', 'Average', 'Min', 'Sum', 'Wind', 'Max', 'Average', 'Min', 'Sea Level Pressure', 'Max', 'Average', 'Min']\n",
      "First Row Sample: ['Max Temperature', '16.0°C (60.8°F)', '9.36°C (48.85°F)', '4.0°C (39.2°F)', '']\n",
      "          Column 1         Column 2          Column 3         Column 4  \\\n",
      "0  Max Temperature  16.0°C (60.8°F)  9.36°C (48.85°F)   4.0°C (39.2°F)   \n",
      "1  Avg Temperature  12.0°C (53.6°F)  7.21°C (44.98°F)   2.0°C (35.6°F)   \n",
      "2  Min Temperature   9.0°C (48.2°F)  3.64°C (38.55°F)  -3.0°C (26.6°F)   \n",
      "3        Dew Point  10.0°C (50.0°F)  4.14°C (39.45°F)  -5.0°C (23.0°F)   \n",
      "4    Precipitation  12.3mm | 0.48in   3.01mm | 0.12in      0.0mm | 0in   \n",
      "\n",
      "          Column 5  \n",
      "0                   \n",
      "1                   \n",
      "2                   \n",
      "3                   \n",
      "4  84.2mm | 3.31in  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL \n",
    "url = \"https://weatherandclimate.com/campania/february-2010\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# parse \n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# locate the table  tb8\n",
    "table = soup.find('table', {'class': 'tb8'})\n",
    "\n",
    "headers = [th.text.strip() for th in table.find_all('th') if th.text.strip()]\n",
    "\n",
    "# get rows\n",
    "rows = []\n",
    "for row in table.find_all('tr'):\n",
    "    cells = row.find_all('td')\n",
    "    if cells:\n",
    "        rows.append([cell.text.strip() for cell in cells])\n",
    "\n",
    "#print(\"Extracted Headers:\", headers)\n",
    "#print(\"First Row Sample:\", rows[0] if rows else \"No data found\")\n",
    "\n",
    "if rows and len(headers) != len(rows[0]):\n",
    "    headers = [f\"Column {i+1}\" for i in range(len(rows[0]))]  # Generate generic column names\n",
    "\n",
    "# dataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Headers: ['Time', 'Temperature', 'Dew Point', 'Humidity', 'Wind Speed', 'Pressure', 'Precipitation']\n",
      "First Row Sample: ['February', '° C | ° F', '° C | ° F', '%', 'Kph | Mph', 'Hg | Mb', 'Total (mm/in)']\n",
      "         Time Temperature  Dew Point Humidity Wind Speed      Pressure  \\\n",
      "0    February   ° C | ° F  ° C | ° F        %  Kph | Mph       Hg | Mb   \n",
      "1  2010-02-01    3 | 37.4  -3 | 26.6       70  10 | 6.21  29.83 | 1010   \n",
      "2  2010-02-02    2 | 35.6  -5 | 23.0       63   7 | 4.35  30.09 | 1019   \n",
      "3  2010-02-03    4 | 39.2  -1 | 30.2       76  10 | 6.21  30.18 | 1022   \n",
      "4  2010-02-04    8 | 46.4   4 | 39.2       85   8 | 4.97  30.21 | 1023   \n",
      "\n",
      "   Precipitation  \n",
      "0  Total (mm/in)  \n",
      "1      0.0 | 0.0  \n",
      "2      0.0 | 0.0  \n",
      "3     0.4 | 0.02  \n",
      "4      0.0 | 0.0  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://weatherandclimate.com/campania/february-2010\"\n",
    "\n",
    "# \n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# locate the table tb7\n",
    "table = soup.find('table', {'class': 'tb7'})\n",
    "\n",
    "# get headers\n",
    "headers = [th.text.strip() for th in table.find_all('th') if th.text.strip()]\n",
    "\n",
    "# Extract rows\n",
    "rows = []\n",
    "for row in table.find_all('tr'):\n",
    "    cells = row.find_all('td')\n",
    "    if cells:\n",
    "        rows.append([cell.text.strip() for cell in cells])\n",
    "\n",
    "\n",
    "#print(\"Extracted Headers:\", headers)\n",
    "#print(\"First Row Sample:\", rows[0] if rows else \"No data found\")\n",
    "\n",
    "if rows and len(headers) != len(rows[0]):\n",
    "    headers = [f\"Column {i+1}\" for i in range(len(rows[0]))]  # Generate generic column names\n",
    "\n",
    "# dataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers type: <class 'dict'>\n",
      "Headers content: {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0'}\n",
      "Scraping: https://weatherandclimate.com/campania/january-2010\n",
      "Scraping: https://weatherandclimate.com/campania/february-2010\n",
      "Scraping: https://weatherandclimate.com/campania/march-2010\n",
      "Scraping: https://weatherandclimate.com/campania/april-2010\n",
      "Scraping: https://weatherandclimate.com/campania/may-2010\n",
      "Scraping: https://weatherandclimate.com/campania/june-2010\n",
      "Scraping: https://weatherandclimate.com/campania/july-2010\n",
      "Scraping: https://weatherandclimate.com/campania/august-2010\n",
      "Scraping: https://weatherandclimate.com/campania/september-2010\n",
      "Scraping: https://weatherandclimate.com/campania/october-2010\n",
      "Scraping: https://weatherandclimate.com/campania/november-2010\n",
      "Scraping: https://weatherandclimate.com/campania/december-2010\n",
      "Scraping: https://weatherandclimate.com/campania/january-2011\n",
      "Scraping: https://weatherandclimate.com/campania/february-2011\n",
      "Scraping: https://weatherandclimate.com/campania/march-2011\n",
      "Scraping: https://weatherandclimate.com/campania/april-2011\n",
      "Scraping: https://weatherandclimate.com/campania/may-2011\n",
      "Scraping: https://weatherandclimate.com/campania/june-2011\n",
      "Scraping: https://weatherandclimate.com/campania/july-2011\n",
      "Scraping: https://weatherandclimate.com/campania/august-2011\n",
      "Scraping: https://weatherandclimate.com/campania/september-2011\n",
      "Scraping: https://weatherandclimate.com/campania/october-2011\n",
      "Scraping: https://weatherandclimate.com/campania/november-2011\n",
      "Scraping: https://weatherandclimate.com/campania/december-2011\n",
      "Scraping: https://weatherandclimate.com/campania/january-2012\n",
      "Scraping: https://weatherandclimate.com/campania/february-2012\n",
      "Scraping: https://weatherandclimate.com/campania/march-2012\n",
      "Scraping: https://weatherandclimate.com/campania/april-2012\n",
      "Scraping: https://weatherandclimate.com/campania/may-2012\n",
      "Scraping: https://weatherandclimate.com/campania/june-2012\n",
      "Scraping: https://weatherandclimate.com/campania/july-2012\n",
      "Scraping: https://weatherandclimate.com/campania/august-2012\n",
      "Scraping: https://weatherandclimate.com/campania/september-2012\n",
      "Scraping: https://weatherandclimate.com/campania/october-2012\n",
      "Scraping: https://weatherandclimate.com/campania/november-2012\n",
      "Scraping: https://weatherandclimate.com/campania/december-2012\n",
      "Scraping: https://weatherandclimate.com/campania/january-2013\n",
      "Scraping: https://weatherandclimate.com/campania/february-2013\n",
      "Scraping: https://weatherandclimate.com/campania/march-2013\n",
      "Scraping: https://weatherandclimate.com/campania/april-2013\n",
      "Scraping: https://weatherandclimate.com/campania/may-2013\n",
      "Scraping: https://weatherandclimate.com/campania/june-2013\n",
      "Scraping: https://weatherandclimate.com/campania/july-2013\n",
      "Scraping: https://weatherandclimate.com/campania/august-2013\n",
      "Scraping: https://weatherandclimate.com/campania/september-2013\n",
      "Scraping: https://weatherandclimate.com/campania/october-2013\n",
      "Scraping: https://weatherandclimate.com/campania/november-2013\n",
      "Scraping: https://weatherandclimate.com/campania/december-2013\n",
      "Scraping: https://weatherandclimate.com/campania/january-2014\n",
      "Scraping: https://weatherandclimate.com/campania/february-2014\n",
      "Scraping: https://weatherandclimate.com/campania/march-2014\n",
      "Scraping: https://weatherandclimate.com/campania/april-2014\n",
      "Scraping: https://weatherandclimate.com/campania/may-2014\n",
      "Scraping: https://weatherandclimate.com/campania/june-2014\n",
      "Scraping: https://weatherandclimate.com/campania/july-2014\n",
      "Scraping: https://weatherandclimate.com/campania/august-2014\n",
      "Scraping: https://weatherandclimate.com/campania/september-2014\n",
      "Scraping: https://weatherandclimate.com/campania/october-2014\n",
      "Scraping: https://weatherandclimate.com/campania/november-2014\n",
      "Scraping: https://weatherandclimate.com/campania/december-2014\n",
      "Scraping: https://weatherandclimate.com/campania/january-2015\n",
      "Scraping: https://weatherandclimate.com/campania/february-2015\n",
      "Scraping: https://weatherandclimate.com/campania/march-2015\n",
      "Scraping: https://weatherandclimate.com/campania/april-2015\n",
      "Scraping: https://weatherandclimate.com/campania/may-2015\n",
      "Scraping: https://weatherandclimate.com/campania/june-2015\n",
      "Scraping: https://weatherandclimate.com/campania/july-2015\n",
      "Scraping: https://weatherandclimate.com/campania/august-2015\n",
      "Scraping: https://weatherandclimate.com/campania/september-2015\n",
      "Scraping: https://weatherandclimate.com/campania/october-2015\n",
      "Scraping: https://weatherandclimate.com/campania/november-2015\n",
      "Scraping: https://weatherandclimate.com/campania/december-2015\n",
      "Scraping: https://weatherandclimate.com/campania/january-2016\n",
      "Scraping: https://weatherandclimate.com/campania/february-2016\n",
      "Scraping: https://weatherandclimate.com/campania/march-2016\n",
      "Scraping: https://weatherandclimate.com/campania/april-2016\n",
      "Scraping: https://weatherandclimate.com/campania/may-2016\n",
      "Scraping: https://weatherandclimate.com/campania/june-2016\n",
      "Scraping: https://weatherandclimate.com/campania/july-2016\n",
      "Scraping: https://weatherandclimate.com/campania/august-2016\n",
      "Scraping: https://weatherandclimate.com/campania/september-2016\n",
      "Scraping: https://weatherandclimate.com/campania/october-2016\n",
      "Scraping: https://weatherandclimate.com/campania/november-2016\n",
      "Scraping: https://weatherandclimate.com/campania/december-2016\n",
      "Scraping: https://weatherandclimate.com/campania/january-2017\n",
      "Scraping: https://weatherandclimate.com/campania/february-2017\n",
      "Scraping: https://weatherandclimate.com/campania/march-2017\n",
      "Scraping: https://weatherandclimate.com/campania/april-2017\n",
      "Scraping: https://weatherandclimate.com/campania/may-2017\n",
      "Scraping: https://weatherandclimate.com/campania/june-2017\n",
      "Scraping: https://weatherandclimate.com/campania/july-2017\n",
      "Scraping: https://weatherandclimate.com/campania/august-2017\n",
      "Scraping: https://weatherandclimate.com/campania/september-2017\n",
      "Scraping: https://weatherandclimate.com/campania/october-2017\n",
      "Scraping: https://weatherandclimate.com/campania/november-2017\n",
      "Scraping: https://weatherandclimate.com/campania/december-2017\n",
      "Scraping: https://weatherandclimate.com/campania/january-2018\n",
      "Scraping: https://weatherandclimate.com/campania/february-2018\n",
      "Scraping: https://weatherandclimate.com/campania/march-2018\n",
      "Scraping: https://weatherandclimate.com/campania/april-2018\n",
      "Scraping: https://weatherandclimate.com/campania/may-2018\n",
      "Scraping: https://weatherandclimate.com/campania/june-2018\n",
      "Scraping: https://weatherandclimate.com/campania/july-2018\n",
      "Scraping: https://weatherandclimate.com/campania/august-2018\n",
      "Scraping: https://weatherandclimate.com/campania/september-2018\n",
      "Scraping: https://weatherandclimate.com/campania/october-2018\n",
      "Scraping: https://weatherandclimate.com/campania/november-2018\n",
      "Scraping: https://weatherandclimate.com/campania/december-2018\n",
      "Scraping: https://weatherandclimate.com/campania/january-2019\n",
      "Scraping: https://weatherandclimate.com/campania/february-2019\n",
      "Scraping: https://weatherandclimate.com/campania/march-2019\n",
      "Scraping: https://weatherandclimate.com/campania/april-2019\n",
      "Scraping: https://weatherandclimate.com/campania/may-2019\n",
      "Scraping: https://weatherandclimate.com/campania/june-2019\n",
      "Scraping: https://weatherandclimate.com/campania/july-2019\n",
      "Scraping: https://weatherandclimate.com/campania/august-2019\n",
      "Scraping: https://weatherandclimate.com/campania/september-2019\n",
      "Scraping: https://weatherandclimate.com/campania/october-2019\n",
      "Scraping: https://weatherandclimate.com/campania/november-2019\n",
      "Scraping: https://weatherandclimate.com/campania/december-2019\n",
      "Data scraping completed and saved as 'weather_data_2010_2019.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define years and months\n",
    "years = range(2010, 2020)  # 2010-2019\n",
    "months = [\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
    "    \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"\n",
    "]\n",
    "\n",
    "# firefox \n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\"\n",
    "}\n",
    "\n",
    "\n",
    "#print(\"Headers type:\", type(headers))  # Should be <class 'dict'>\n",
    "#print(\"Headers content:\", headers)\n",
    "\n",
    "# store data\n",
    "all_data = []\n",
    "\n",
    "# each year and month\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        url = f\"https://weatherandclimate.com/campania/{month}-{year}\"\n",
    "        print(f\"Scraping: {url}\")\n",
    "\n",
    "        try:\n",
    "            \n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to retrieve {url}: {e}\")\n",
    "            continue  \n",
    "\n",
    "        # parse\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # tables\n",
    "        tables = soup.find_all('table')\n",
    "\n",
    "        # second table\n",
    "        if len(tables) < 2:\n",
    "            print(f\"No second table found for {month} {year}\")\n",
    "            continue\n",
    "        table = tables[1]\n",
    "\n",
    "        \n",
    "        table_headers = [th.text.strip() for th in table.find_all('th') if th.text.strip()]\n",
    "\n",
    "        \n",
    "        rows = []\n",
    "        for row in table.find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            if cells:\n",
    "                rows.append([cell.text.strip() for cell in cells])\n",
    "\n",
    "        # dataFrame\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows, columns=table_headers)\n",
    "            df[\"Month\"] = month\n",
    "            df[\"Year\"] = year\n",
    "            all_data.append(df)\n",
    "\n",
    "        # pause \n",
    "        time.sleep(2)\n",
    "\n",
    "# combine all data\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df.to_csv(\"weather_data_avg_2010_2019.csv\", index=False)\n",
    "    print(\"Data scraping completed and saved as 'weather_data_2010_2019.csv'.\")\n",
    "else:\n",
    "    print(\"No data was collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers type: <class 'dict'>\n",
      "Scraping: https://weatherandclimate.com/campania/january-2010\n",
      "Scraping: https://weatherandclimate.com/campania/february-2010\n",
      "Scraping: https://weatherandclimate.com/campania/march-2010\n",
      "Scraping: https://weatherandclimate.com/campania/april-2010\n",
      "Scraping: https://weatherandclimate.com/campania/may-2010\n",
      "Scraping: https://weatherandclimate.com/campania/june-2010\n",
      "Scraping: https://weatherandclimate.com/campania/july-2010\n",
      "Scraping: https://weatherandclimate.com/campania/august-2010\n",
      "Scraping: https://weatherandclimate.com/campania/september-2010\n",
      "Scraping: https://weatherandclimate.com/campania/october-2010\n",
      "Scraping: https://weatherandclimate.com/campania/november-2010\n",
      "Scraping: https://weatherandclimate.com/campania/december-2010\n",
      "Scraping: https://weatherandclimate.com/campania/january-2011\n",
      "Scraping: https://weatherandclimate.com/campania/february-2011\n",
      "Scraping: https://weatherandclimate.com/campania/march-2011\n",
      "Scraping: https://weatherandclimate.com/campania/april-2011\n",
      "Scraping: https://weatherandclimate.com/campania/may-2011\n",
      "Scraping: https://weatherandclimate.com/campania/june-2011\n",
      "Scraping: https://weatherandclimate.com/campania/july-2011\n",
      "Scraping: https://weatherandclimate.com/campania/august-2011\n",
      "Scraping: https://weatherandclimate.com/campania/september-2011\n",
      "Scraping: https://weatherandclimate.com/campania/october-2011\n",
      "Scraping: https://weatherandclimate.com/campania/november-2011\n",
      "Scraping: https://weatherandclimate.com/campania/december-2011\n",
      "Scraping: https://weatherandclimate.com/campania/january-2012\n",
      "Scraping: https://weatherandclimate.com/campania/february-2012\n",
      "Scraping: https://weatherandclimate.com/campania/march-2012\n",
      "Scraping: https://weatherandclimate.com/campania/april-2012\n",
      "Scraping: https://weatherandclimate.com/campania/may-2012\n",
      "Scraping: https://weatherandclimate.com/campania/june-2012\n",
      "Scraping: https://weatherandclimate.com/campania/july-2012\n",
      "Scraping: https://weatherandclimate.com/campania/august-2012\n",
      "Scraping: https://weatherandclimate.com/campania/september-2012\n",
      "Scraping: https://weatherandclimate.com/campania/october-2012\n",
      "Scraping: https://weatherandclimate.com/campania/november-2012\n",
      "Scraping: https://weatherandclimate.com/campania/december-2012\n",
      "Scraping: https://weatherandclimate.com/campania/january-2013\n",
      "Scraping: https://weatherandclimate.com/campania/february-2013\n",
      "Scraping: https://weatherandclimate.com/campania/march-2013\n",
      "Scraping: https://weatherandclimate.com/campania/april-2013\n",
      "Scraping: https://weatherandclimate.com/campania/may-2013\n",
      "Scraping: https://weatherandclimate.com/campania/june-2013\n",
      "Scraping: https://weatherandclimate.com/campania/july-2013\n",
      "Scraping: https://weatherandclimate.com/campania/august-2013\n",
      "Scraping: https://weatherandclimate.com/campania/september-2013\n",
      "Scraping: https://weatherandclimate.com/campania/october-2013\n",
      "Scraping: https://weatherandclimate.com/campania/november-2013\n",
      "Scraping: https://weatherandclimate.com/campania/december-2013\n",
      "Scraping: https://weatherandclimate.com/campania/january-2014\n",
      "Scraping: https://weatherandclimate.com/campania/february-2014\n",
      "Scraping: https://weatherandclimate.com/campania/march-2014\n",
      "Scraping: https://weatherandclimate.com/campania/april-2014\n",
      "Scraping: https://weatherandclimate.com/campania/may-2014\n",
      "Scraping: https://weatherandclimate.com/campania/june-2014\n",
      "Scraping: https://weatherandclimate.com/campania/july-2014\n",
      "Scraping: https://weatherandclimate.com/campania/august-2014\n",
      "Scraping: https://weatherandclimate.com/campania/september-2014\n",
      "Scraping: https://weatherandclimate.com/campania/october-2014\n",
      "Scraping: https://weatherandclimate.com/campania/november-2014\n",
      "Scraping: https://weatherandclimate.com/campania/december-2014\n",
      "Scraping: https://weatherandclimate.com/campania/january-2015\n",
      "Scraping: https://weatherandclimate.com/campania/february-2015\n",
      "Scraping: https://weatherandclimate.com/campania/march-2015\n",
      "Scraping: https://weatherandclimate.com/campania/april-2015\n",
      "Scraping: https://weatherandclimate.com/campania/may-2015\n",
      "Scraping: https://weatherandclimate.com/campania/june-2015\n",
      "Scraping: https://weatherandclimate.com/campania/july-2015\n",
      "Scraping: https://weatherandclimate.com/campania/august-2015\n",
      "Scraping: https://weatherandclimate.com/campania/september-2015\n",
      "Scraping: https://weatherandclimate.com/campania/october-2015\n",
      "Scraping: https://weatherandclimate.com/campania/november-2015\n",
      "Scraping: https://weatherandclimate.com/campania/december-2015\n",
      "Scraping: https://weatherandclimate.com/campania/january-2016\n",
      "Scraping: https://weatherandclimate.com/campania/february-2016\n",
      "Scraping: https://weatherandclimate.com/campania/march-2016\n",
      "Scraping: https://weatherandclimate.com/campania/april-2016\n",
      "Scraping: https://weatherandclimate.com/campania/may-2016\n",
      "Scraping: https://weatherandclimate.com/campania/june-2016\n",
      "Scraping: https://weatherandclimate.com/campania/july-2016\n",
      "Scraping: https://weatherandclimate.com/campania/august-2016\n",
      "Scraping: https://weatherandclimate.com/campania/september-2016\n",
      "Scraping: https://weatherandclimate.com/campania/october-2016\n",
      "Scraping: https://weatherandclimate.com/campania/november-2016\n",
      "Scraping: https://weatherandclimate.com/campania/december-2016\n",
      "Scraping: https://weatherandclimate.com/campania/january-2017\n",
      "Scraping: https://weatherandclimate.com/campania/february-2017\n",
      "Scraping: https://weatherandclimate.com/campania/march-2017\n",
      "Scraping: https://weatherandclimate.com/campania/april-2017\n",
      "Scraping: https://weatherandclimate.com/campania/may-2017\n",
      "Scraping: https://weatherandclimate.com/campania/june-2017\n",
      "Scraping: https://weatherandclimate.com/campania/july-2017\n",
      "Scraping: https://weatherandclimate.com/campania/august-2017\n",
      "Scraping: https://weatherandclimate.com/campania/september-2017\n",
      "Scraping: https://weatherandclimate.com/campania/october-2017\n",
      "Scraping: https://weatherandclimate.com/campania/november-2017\n",
      "Scraping: https://weatherandclimate.com/campania/december-2017\n",
      "Scraping: https://weatherandclimate.com/campania/january-2018\n",
      "Scraping: https://weatherandclimate.com/campania/february-2018\n",
      "Scraping: https://weatherandclimate.com/campania/march-2018\n",
      "Scraping: https://weatherandclimate.com/campania/april-2018\n",
      "Scraping: https://weatherandclimate.com/campania/may-2018\n",
      "Scraping: https://weatherandclimate.com/campania/june-2018\n",
      "Scraping: https://weatherandclimate.com/campania/july-2018\n",
      "Scraping: https://weatherandclimate.com/campania/august-2018\n",
      "Scraping: https://weatherandclimate.com/campania/september-2018\n",
      "Scraping: https://weatherandclimate.com/campania/october-2018\n",
      "Scraping: https://weatherandclimate.com/campania/november-2018\n",
      "Scraping: https://weatherandclimate.com/campania/december-2018\n",
      "Scraping: https://weatherandclimate.com/campania/january-2019\n",
      "Scraping: https://weatherandclimate.com/campania/february-2019\n",
      "Scraping: https://weatherandclimate.com/campania/march-2019\n",
      "Scraping: https://weatherandclimate.com/campania/april-2019\n",
      "Scraping: https://weatherandclimate.com/campania/may-2019\n",
      "Scraping: https://weatherandclimate.com/campania/june-2019\n",
      "Scraping: https://weatherandclimate.com/campania/july-2019\n",
      "Scraping: https://weatherandclimate.com/campania/august-2019\n",
      "Scraping: https://weatherandclimate.com/campania/september-2019\n",
      "Scraping: https://weatherandclimate.com/campania/october-2019\n",
      "Scraping: https://weatherandclimate.com/campania/november-2019\n",
      "Scraping: https://weatherandclimate.com/campania/december-2019\n",
      "First table data saved as 'scraped_weather_data\\weather_data_first_table_2010_2019.csv'.\n",
      "Second table data saved as 'scraped_weather_data\\weather_data_avg_table_2010_2019.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# years and months\n",
    "years = range(2010, 2020)  # 2010-2019\n",
    "months = [\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
    "    \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"\n",
    "]\n",
    "\n",
    "# firefox\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\"\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Headers type:\", type(headers))  \n",
    "\n",
    "\n",
    "first_table_data = []\n",
    "second_table_data = []\n",
    "\n",
    "# year and month\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        url = f\"https://weatherandclimate.com/campania/{month}-{year}\"\n",
    "        print(f\"Scraping: {url}\")\n",
    "\n",
    "        try:\n",
    "            # request\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to retrieve {url}: {e}\")\n",
    "            continue  \n",
    "\n",
    "        # parse\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all tables\n",
    "        tables = soup.find_all('table')\n",
    "\n",
    "        if not tables:\n",
    "            print(f\"No tables found for {month} {year}\")\n",
    "            continue\n",
    "\n",
    "        # Process the first table if it exists\n",
    "        if len(tables) > 0:\n",
    "            first_table = tables[0]\n",
    "            first_headers = [th.text.strip() for th in first_table.find_all('th') if th.text.strip()]\n",
    "            first_rows = [\n",
    "                [td.text.strip() for td in row.find_all('td')]\n",
    "                for row in first_table.find_all('tr')\n",
    "                if row.find_all('td')\n",
    "            ]\n",
    "\n",
    "            \n",
    "            if first_headers and first_rows:\n",
    "                min_cols = min(len(first_headers), len(first_rows[0]))\n",
    "                df_first = pd.DataFrame(first_rows, columns=first_headers[:min_cols])  # Trim headers if needed\n",
    "                df_first[\"Month\"] = month\n",
    "                df_first[\"Year\"] = year\n",
    "                first_table_data.append(df_first)\n",
    "\n",
    "       \n",
    "        if len(tables) > 1:\n",
    "            second_table = tables[1]\n",
    "            second_headers = [th.text.strip() for th in second_table.find_all('th') if th.text.strip()]\n",
    "            second_rows = [\n",
    "                [td.text.strip() for td in row.find_all('td')]\n",
    "                for row in second_table.find_all('tr')\n",
    "                if row.find_all('td')\n",
    "            ]\n",
    "\n",
    "            \n",
    "            if second_headers and second_rows:\n",
    "                min_cols = min(len(second_headers), len(second_rows[0]))\n",
    "                df_second = pd.DataFrame(second_rows, columns=second_headers[:min_cols])\n",
    "                df_second[\"Month\"] = month\n",
    "                df_second[\"Year\"] = year\n",
    "                second_table_data.append(df_second)\n",
    "\n",
    "        # pause\n",
    "        time.sleep(2)\n",
    "\n",
    "# save data\n",
    "output_dir = \"scraped_weather_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if first_table_data:\n",
    "    final_df_first = pd.concat(first_table_data, ignore_index=True)\n",
    "    first_table_path = os.path.join(output_dir, \"weather_data_first_table_2010_2019.csv\")\n",
    "    final_df_first.to_csv(first_table_path, index=False)\n",
    "    print(f\"First table data saved as '{first_table_path}'.\")\n",
    "\n",
    "if second_table_data:\n",
    "    final_df_second = pd.concat(second_table_data, ignore_index=True)\n",
    "    second_table_path = os.path.join(output_dir, \"weather_data_avg_table_2010_2019.csv\")\n",
    "    final_df_second.to_csv(second_table_path, index=False)\n",
    "    print(f\"Second table data saved as '{second_table_path}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
